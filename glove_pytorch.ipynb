{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x762c2d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of tokens:  5526 \n",
      " ['the', 'last', 'question', 'was', 'asked', 'for', 'the', 'first', 'time', ',']\n"
     ]
    }
   ],
   "source": [
    "# Open and read in text\n",
    "text_file = open('short_story.txt', 'r')\n",
    "raw_text = text_file.read().lower()\n",
    "text_file.close()\n",
    "\n",
    "# Create tokenized text (list) and vocabulary (set of unique words)\n",
    "token_text = word_tokenize(raw_text)\n",
    "len_token_text = len(token_text)\n",
    "\n",
    "print(\"# of tokens: \", len(token_text), '\\n', token_text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary:  1115\n"
     ]
    }
   ],
   "source": [
    "# set of vocab items\n",
    "vocab = set(token_text)\n",
    "vocab_size = len(vocab)\n",
    "print(\"size of vocabulary: \", vocab_size)\n",
    "\n",
    "# dictionaries mapping from word to index and vica versa\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i: word for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of co-occurrence matrix: (1115, 1115)\n",
      "non-zero co-occurrences:\n",
      " [[   0   12]\n",
      " [   0   41]\n",
      " [   0   55]\n",
      " ...\n",
      " [1114  701]\n",
      " [1114  881]\n",
      " [1114 1025]]\n"
     ]
    }
   ],
   "source": [
    "# \"Unless otherwise noted, we use a context of ten words to the left and\n",
    "# ten words to the right.\"\n",
    "CONTEXT_SIZE = 10\n",
    "\n",
    "# Construct co-occurence matrix\n",
    "co_occ_mat = np.zeros((vocab_size, vocab_size))\n",
    "for i in range(len_token_text):\n",
    "    for dist in range(1, CONTEXT_SIZE + 1):\n",
    "        ix = word_to_ix[token_text[i]]\n",
    "        if i - dist > 0:\n",
    "            left_ix = word_to_ix[token_text[i - dist]]\n",
    "            co_occ_mat[ix, left_ix] += 1.0 / dist\n",
    "        if i + dist < len_token_text:\n",
    "            right_ix = word_to_ix[token_text[i + dist]]\n",
    "            co_occ_mat[ix, right_ix] += 1.0 / dist\n",
    "\n",
    "# Non-zero co-occurrences\n",
    "# https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.nonzero.html\n",
    "# returns a 2-D array, with a row for each non-zero element            \n",
    "co_occs = np.transpose(np.nonzero(co_occ_mat))\n",
    "\n",
    "print(\"shape of co-occurrence matrix:\", co_occ_mat.shape)\n",
    "print(\"non-zero co-occurrences:\\n\", co_occs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#global variables\n",
    "\n",
    "EMBEDDING_SIZE = 50\n",
    "\n",
    "# \"For all our experiments, we set x_max = 100, alpha = 3/4\"\n",
    "X_MAX = 100\n",
    "ALPHA = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Glove(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, comat, embedding_size, x_max, alpha):\n",
    "        super(Glove, self).__init__()\n",
    "        \n",
    "        # embedding matrices\n",
    "        self.embedding_V = nn.Embedding(vocab_size, embedding_size) # embedding matrix of center words\n",
    "        self.embedding_U = nn.Embedding(vocab_size, embedding_size) # embedding matrix of context words\n",
    "\n",
    "        # biases\n",
    "        self.v_bias = nn.Embedding(vocab_size, 1)\n",
    "        self.u_bias = nn.Embedding(vocab_size, 1)\n",
    "        \n",
    "        # initialize all params\n",
    "        for params in self.parameters():\n",
    "            nn.init.uniform_(params, a = -0.5, b = 0.5)\n",
    "            \n",
    "        #hyperparams\n",
    "        self.x_max = x_max\n",
    "        self.alpha = alpha\n",
    "        self.comat = comat\n",
    "    \n",
    "    def forward(self, center_word_lookup, context_word_lookup):\n",
    "        # indexing into the embedding matrices\n",
    "        center_embed = self.embedding_V(center_word_lookup)\n",
    "        target_embed = self.embedding_U(context_word_lookup)\n",
    "\n",
    "        center_bias = self.v_bias(center_word_lookup).squeeze(1)\n",
    "        target_bias = self.u_bias(context_word_lookup).squeeze(1)\n",
    "\n",
    "        # elements of the co-occurence matrix\n",
    "        co_occurrences = torch.tensor([self.comat[center_word_lookup[i].item(), context_word_lookup[i].item()]\n",
    "                                       for i in range(BATCH_SIZE)])\n",
    "        \n",
    "        # weight_fn applied to non-zero co-occurrences\n",
    "        weights = torch.tensor([self.weight_fn(var) for var in co_occurrences])\n",
    "\n",
    "        # the loss as described in the paper\n",
    "        loss = torch.sum(torch.pow((torch.sum(center_embed * target_embed, dim=1)\n",
    "            + center_bias + target_bias) - torch.log(co_occurrences), 2) * weights)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def weight_fn(self, x):\n",
    "        # the proposed weighting fn\n",
    "        if x < self.x_max:\n",
    "            return (x / self.x_max) ** self.alpha\n",
    "        return 1\n",
    "        \n",
    "    def embeddings(self):\n",
    "        # \"we choose to use the sum W + W_tilde as our word vectors\"\n",
    "        return self.embedding_V.weight.data + self.embedding_U.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# Batch sampling function\n",
    "def gen_batch(model, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    picks random indices for lookup in the embedding matrix\n",
    "    \"stochastically sampling non-zero elements from X [ie. the co-occurrence matrix]\"\n",
    "    \"\"\"\t\n",
    "    sample = np.random.choice(np.arange(len(co_occs)), size=batch_size, replace=False)\n",
    "    v_vecs_ix, u_vecs_ix = [], []\n",
    "    \n",
    "    for chosen in sample:\n",
    "        ind = tuple(co_occs[chosen])     \n",
    "        lookup_ix_v = ind[0]\n",
    "        lookup_ix_u = ind[1]\n",
    "        \n",
    "        v_vecs_ix.append(lookup_ix_v)\n",
    "        u_vecs_ix.append(lookup_ix_u) \n",
    "        \n",
    "    return torch.tensor(v_vecs_ix), torch.tensor(u_vecs_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"[we] train the model using AdaGrad, [...] with initial learning rate of 0.05\"\n",
    "LEARNING_RATE = 0.05\n",
    "\n",
    "# \"we run 50 iterations for vectors smaller than 300 dimensions [...]\"\n",
    "EPOCHS = 50\n",
    "\n",
    "def train_glove(comat):\n",
    "    losses = []\n",
    "    model = Glove(vocab_size, comat, embedding_size=EMBEDDING_SIZE, x_max=X_MAX, alpha=ALPHA)\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr = LEARNING_RATE)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "        num_batches = int(len_token_text/BATCH_SIZE)\n",
    "        print(\"Beginning epoch %d\" %epoch)\n",
    "        for batch in tqdm(range(num_batches)):\n",
    "            model.zero_grad()\n",
    "            data = gen_batch(model, BATCH_SIZE)\n",
    "            loss = model(*data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        losses.append(total_loss)\n",
    "        print('Epoch : %d, mean loss : %.02f' % (epoch, np.mean(losses)))\n",
    "    return model, losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 113.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, mean loss : 205.01\n",
      "Beginning epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 120.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, mean loss : 204.41\n",
      "Beginning epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 120.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2, mean loss : 200.28\n",
      "Beginning epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 120.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3, mean loss : 195.47\n",
      "Beginning epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 172/172 [00:01<00:00, 86.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4, mean loss : 195.80\n",
      "Beginning epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 100.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 5, mean loss : 188.49\n",
      "Beginning epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 102.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 6, mean loss : 186.98\n",
      "Beginning epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 115.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 7, mean loss : 178.71\n",
      "Beginning epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 116.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 8, mean loss : 173.80\n",
      "Beginning epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 112.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 9, mean loss : 166.81\n",
      "Beginning epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 112.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 10, mean loss : 161.67\n",
      "Beginning epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 116.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 11, mean loss : 158.75\n",
      "Beginning epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 172/172 [00:02<00:00, 84.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 12, mean loss : 153.76\n",
      "Beginning epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 109.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 13, mean loss : 150.61\n",
      "Beginning epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 106.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 14, mean loss : 146.78\n",
      "Beginning epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 114.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 15, mean loss : 144.27\n",
      "Beginning epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 111.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 16, mean loss : 140.59\n",
      "Beginning epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 112.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 17, mean loss : 138.14\n",
      "Beginning epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 110.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 18, mean loss : 135.28\n",
      "Beginning epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 110.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 19, mean loss : 133.32\n",
      "Beginning epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 115.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 20, mean loss : 130.29\n",
      "Beginning epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 107.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 21, mean loss : 127.46\n",
      "Beginning epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 114.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 22, mean loss : 124.78\n",
      "Beginning epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 115.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 23, mean loss : 122.25\n",
      "Beginning epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 106.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 24, mean loss : 120.11\n",
      "Beginning epoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 100.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 25, mean loss : 117.81\n",
      "Beginning epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 117.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 26, mean loss : 115.85\n",
      "Beginning epoch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 122.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 27, mean loss : 113.76\n",
      "Beginning epoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 123.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 28, mean loss : 111.81\n",
      "Beginning epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 120.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 29, mean loss : 109.99\n",
      "Beginning epoch 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 119.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 30, mean loss : 108.17\n",
      "Beginning epoch 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 104.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 31, mean loss : 106.56\n",
      "Beginning epoch 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 111.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 32, mean loss : 104.88\n",
      "Beginning epoch 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 113.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 33, mean loss : 103.37\n",
      "Beginning epoch 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 103.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 34, mean loss : 101.96\n",
      "Beginning epoch 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 103.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 35, mean loss : 100.53\n",
      "Beginning epoch 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 119.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 36, mean loss : 99.03\n",
      "Beginning epoch 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 108.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 37, mean loss : 97.61\n",
      "Beginning epoch 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 172/172 [00:01<00:00, 99.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 38, mean loss : 96.38\n",
      "Beginning epoch 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 117.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 39, mean loss : 95.09\n",
      "Beginning epoch 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 103.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 40, mean loss : 93.89\n",
      "Beginning epoch 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 117.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 41, mean loss : 92.70\n",
      "Beginning epoch 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 120.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 42, mean loss : 91.55\n",
      "Beginning epoch 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 123.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 43, mean loss : 90.48\n",
      "Beginning epoch 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 121.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 44, mean loss : 89.42\n",
      "Beginning epoch 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 119.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 45, mean loss : 88.36\n",
      "Beginning epoch 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 111.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 46, mean loss : 87.33\n",
      "Beginning epoch 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 107.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 47, mean loss : 86.38\n",
      "Beginning epoch 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 109.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 48, mean loss : 85.42\n",
      "Beginning epoch 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 172/172 [00:01<00:00, 111.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 49, mean loss : 84.52\n"
     ]
    }
   ],
   "source": [
    "model, losses = train_glove(co_occ_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEZCAYAAABiu9n+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npU8DvsjOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x100c1b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10034828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss fn\n",
    "def plot_loss_fn(losses, title):\n",
    "    plt.plot(range(len(losses)), losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(title)\n",
    "    plt.figure()\n",
    "\n",
    "plot_loss_fn(losses, \"GloVe loss function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Tests: word similarity, word analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word(word, model, word_to_ix):\n",
    "    \"\"\"\n",
    "    returns the embedding that belongs to the given word (str)\n",
    "    \"\"\"\n",
    "    return model.embeddings()[word_to_ix[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.3648, -0.0519, -0.0263, -0.5178,  0.0834, -0.0161,  0.5632,  0.6154,\n",
      "        -0.5049,  0.2271,  0.3217,  0.0170,  0.0616,  0.1479,  0.6156, -0.0939,\n",
      "        -0.2456,  0.0683,  0.1398,  0.2605,  0.0317, -0.3788,  0.0700,  0.8735,\n",
      "         0.4004,  0.4227, -0.2359,  0.1729, -0.4364, -0.0402, -0.3067, -0.6184,\n",
      "         0.3040, -0.0342,  0.3564,  0.2884,  0.1523,  0.8375,  0.3730,  0.3199,\n",
      "        -0.1104, -0.2765, -0.1177,  0.3219,  0.3092, -0.6014, -0.5400, -0.7912,\n",
      "        -0.5602,  0.0829])\n"
     ]
    }
   ],
   "source": [
    "vector = get_word(\"lupov\", model, word_to_ix)\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def closest(vec, word_to_ix, n=10):\n",
    "    \"\"\"\n",
    "    finds the closest words for a given vector\n",
    "    \"\"\"\n",
    "    all_dists = [(w, torch.dist(vec, get_word(w, model, word_to_ix))) for w in word_to_ix]\n",
    "    return sorted(all_dists, key=lambda t: t[1])[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lupov', tensor(0.)),\n",
       " ('two', tensor(2.9694)),\n",
       " ('world', tensor(3.0176)),\n",
       " ('clickings', tensor(3.0212)),\n",
       " ('circuits', tensor(3.0248)),\n",
       " ('man', tensor(3.0657)),\n",
       " ('when', tensor(3.0771)),\n",
       " ('still', tensor(3.1216)),\n",
       " ('questions', tensor(3.2087)),\n",
       " ('face', tensor(3.2146))]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest(vector, word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some helper fn\n",
    "def print_tuples(tuples):\n",
    "    for tuple in tuples:\n",
    "        print('(%.4f) %s' % (tuple[1], tuple[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word analogies in the form w1 : w2 :: w3 : ?\n",
    "def analogy(w1, w2, w3, n=5, filter_given=True):\n",
    "    print('\\n[%s : %s :: %s : ?]' % (w1, w2, w3))\n",
    "   \n",
    "    # w2 - w1 + w3 = w4\n",
    "    closest_words = closest(get_word(w2, model, word_to_ix) - get_word(w1, model, word_to_ix) + get_word(w3, model, word_to_ix), word_to_ix)\n",
    "    \n",
    "    # Optionally filter out given words\n",
    "    if filter_given:\n",
    "        closest_words = [t for t in closest_words if t[0] not in [w1, w2, w3]]\n",
    "        \n",
    "    print_tuples(closest_words[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[lupov : adell :: multivac : ?]\n",
      "(4.2572) you\n",
      "(4.2825) at\n",
      "(4.4366) i\n",
      "(4.5461) that\n",
      "(4.5582) now\n"
     ]
    }
   ],
   "source": [
    "analogy(\"lupov\", \"adell\", \"multivac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
